{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CsCpsbld/MLDLAI_1/blob/main/LLM/VLM/Gemma/lora_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result"
      ],
      "metadata": {
        "id": "ckNJE-RJMIhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAh8AAABtCAIAAACOSpRlAAAQAElEQVR4Aex9D1hTV5r3S5f5JM/YJdmxNne+IoRVmlD/ha2l8BS6poNTw9Jug9CVDPRrA93WRHYqYWZHo/usBme3gJ0PwXaEtFuY4FcV/CpDnEoNY+xHpPQJ/qtJ0SUg7l5qnU3YOk/i08zDd87NHwgJCAhJhHOfc/+99z3vn9+53Peec8N5H/rM+P9Gw7cMDg6GT/kcaw6jL2FUHQhiRBkTaF5QyoNoc1BHpiASH6cAh1yaWwTcN9tDAHAjfEt4tc+t32H0JYyqAzGMKGMCzQtKeRBtDurIFETi4xTgkEtzi4D7ZsPRZQVZCAKLGwH0xzB7AEhNggBBwB8B9x8Uji7oiBSCAEGAIEAQIAjMIQIkuswhmEQUQYAgQBAgCHgQmH508VQgO4IAQWChImCtTI4q0oXGO11RlLQtQJXT6QygeQguc3UGh0Nl1w14CDPdBdcYXMoUdgSvMI5qrV6TXH19HGGxHpLoslhbnvi92BGw6/c8R7FYHBaLklQb7REBh24bK7dpElNOqZU8rY1ulyfMtakWTe4qFLcotOZWMkicUrAkzZPYMdfaF648El0WbtsSzyINgUiyx/qOSDogN3/rsDkcvVm69KKIeJiK3x9tL2QHxclq6RWsEQS9dH9Es2pzheCIzUbTtm/M4s50OepRbW4Y1RUEt+P+lC2q2iS6LKrmJs4SBNwI9Gredir3SNjR+JT7ZtWuHk3LMD6GXvVzcRTFYiXv0OOXd5dZI6Y4FHqvT1F2YgLcbJauoqg4ihIqMUebNEosLaaikis/r0tLVF1hhHyhotLqkLzhI9JECi8pHmnDzUUU6ixRAqnGynD6b7yDVzppVLZiR3oij8NipVdbADqU6XvM5j3plLjOCnYjOqDQkphbbwbAI1HS19KjoqRoUG+mGgHs9mEuZxljRzRbphvV5gAgp5gRQvt5VTqLxYmjUsTPUWuqrRBgGIC9U5mCbEEIZVT03mHkuDeB0Lnpi2ZLosuiaWriKEFgDAHaOiwUrPSdsznLunrdnwriZC1Wmv7WJO3IVn0Gwx8oVKtb8Xt9R25XpdYKZlWGOrmNpodoc6k5e48Ri+hlFZhHr5Y/lVvAbT2Fg4b5ZHPya7ncK6r0/cntQzRNm+UWLM15TC531dEOmr6sSmFCFa4efDXFSPT9Vpv1bVAdMkJWVddegWBvF62TU0dk4utKM03TQ1reHoUGBTEwW4Wto6Na8Ww0pqmPJletotJfr279Ytj/q4+5amtrdq/DNkTrS7hee/0Ng966HTRjjU2XViurx+67HQqAzk1eRNv5iC6LCD7iKkFgoSGwjMIdmmhBQRFPb7RyN0p49bLsPXU6kOl1ct6ATjdgrsrCfRfBTr3T2IWfpssEPGYUifuSlN3YagWr7gSvIIdr7dBZLVUiHmIWqDqdXT1WfUerOE+CeaMFQuHUyHEobgzi4CbwnHYbOvAVJMR+Qi5AnSeeRHNbb+zFV3gJXLSbnUZ2TgP9bW9dFrSU8Dhp1WYXksQU5GyMVMLHx+w1KTy8R+sEw4TyA1TVGuQjJTk47PTVBeBOgA5VXWSFRJdF1uDEXYIARoDicXvN7s4KPrXbbqcLx7oymOR5UK6Ud9FdVRs55rfTOZs0wy5w8tVdqDuCCu0Y7SnzPnNxFXgsV/aIprVT18KTSdHT3uXEvQ3EiYpj1PSWHy9TYVYbFxQ0o44LLrbvRhs2jxMya40xXGFembaXruUoFR/g3tA4oVMf6pSbzMrL2BjUu/JjnQCd37VFcUKiy6JoZuLkA4bAvJsrlP0spmpvq5151x5+V1mxQZaL4gHSe5vGRJe5tdkqSuMZS1np9U7BxoKyw7tEHUbTSrHEqdGcZwaQbht1573DRagiLlxpCU8tUXG2SmIAeJslzsMaI8Nr/0xntIMoS6I71orruMy9TJ8DV5rhKtosaf5Ag40Ep/mUHgU8n4DZaBzWPMeStt5mZDBosJeymROABLHYqW1FX33Qx5XLJtxL81wYt3M5HS5wIILLrj+HPgKhI08xToDOQ15EOxJdFlFjE1cJAj4EeG/ptQl1gofxL5KFHeKuRs9PpARsYy4ay3o4RZvVrn4G0va2p78v4KCv1oI66qRaDAL1qV3WPAp96KbSqmg22yfQfRCTI023i6Q5KLgArFa377RKKFw75QDNXgoxWzXapXKKRVFr1CamqrUyObky+HPbLTBwi4ScWVnHe4SiKJ60A2KYHyZ42Kah0cPp23FlLa28ujQOspLzCK+Of0azNcZ7ETkraxfir/qi+mG2j+y9jPfREtVhUD2CeMRGtsA6NNbvCYAOsy+qlUSXRdXcxFmCgA8BtmjvGdqBf5FMt5alMc96XvnVq80NZ/BAluPqARGmsUVVvfjXujRtashhejd8WcuQDX3opq+1yNA3iRzt6OVx42NLC9pHWwqWerQISlpopnZ/q0yAwwBbfBjpRJ/5tS3n8K+z7DYb7zHKww0gbsREtNeOXi1zj9Qh+Y1ixIBtK3ePrbFFB0yMVNqEjeSVXXbXQlxwT42YyX9lb1afuYbl2Ww2RiCAV6nZzNN860DOtmQ57GnpPBAHGiYoaae/c9jorobGq46aNABkD2N8IHT+ehf8WXijy4KHlzhIECAITIGAWX8hW54TtFMwRa3QXeKAThqHOkmclMb09r0ocoRO9QLQRKLLAmhE4gJB4AFFQFCmaxB7OzoR6AP3pQYTjRabrbeK6cpFoI2RaxKJLpHbNsQygsC9ESAcBIFIRYBEl0htGWIXQYAgQBB4kBEg0eVBbj1iO0GAIEAQiFQEcHS58gAsxESCwDwi8N///d83yEIQIAjMEQIo3iFJOLo8Fr4FGRE+5XOsOYy+hFF1IIgRZUygeUEpyGb/zK0L8Iz4uAAbNVJdct9sOLqgI1IIAgSBBY4AcY8gEFoESHQJLd5EG0GAIEAQWBwIkOgSvJ3v3g1On2dqRwkntbZ/npVMT/xgTSrnjY7p8RIuggBBICQIuMwaiUoPOmnUPCdXHqhTvOM3bdos3POPLr8r4XAo/hN8Pp+inihqMIf2EevTnpCwvrC2Z2QW7gDAzGuNGHaLEhIS+PyEhGd/2nbLBXCjdtO63ZdmLmmshs+X0CPpU43aEZVdhjGrQn+EjEmvHfTpnXDqo0fUQZs0ak31zKa+iij7p2WMXb8jhYP+2ikOR1jcetNb505zbhSl+sJ7OmF/RZUYld08PkHWBIaIOp3QjhNO58NUiyZ3lRtTKrfSiCfrnGstvXuk+kKlaK7FMvLwLcFi4ZsC52RLkMu+kRWfYqYgZS7PYuMfXZAAwa5PvrRYLPTAh/G1f1fZhyihLG7tA199/PznhS+Neyr52YBe8Etm/VKNXslTa8YedwB3jyte/Ep+cWDAYhn4qur75TmHBmGF4qxl31o/pTM/cfsSFiTdqlE7olKROXPTSY0FjoDziOy560orM7cWfZAt3+yJpsNHNL18Tt37+qD+Gw/XxfDNtTOboD6opAVJDJZBeW4dvalRnpdVv8SenlRr9Rpv/wZFVibV5lQVr2sUJ8Rd39ps1mrnNrXOBcKdCtsO9f30XwKii1f/kiczN9wYHIRxj3L04omHShAlv3zXpvXrEihqU+01XGHk3O5U9PKPSmFTH3r3x7T7WZfES6uUf1bb1Avg6msqXI86U0h2ScstuNGQ/0TJcThe8sRu9E6O9D6LrqFOx+bqS39EGkcMuzyGFH3ojowjPfs3YRb+ekQx7OL/+Jd9fb/8cf77vgAz4hiJ4ca6ZzpasqHC0l0aD/21qfiN27AbvfszBb3hPYuiDsCtlhJsDZ//7C7DNDtXXiQhwNogSCIfcBnpKOE/68YWn97figNqcemLT/ATHuGkes3u+7AIwYaQ8XTXABBlomuXaosy16PeF/+nHdhZV19THt/dx9t9DhNmaZdZvX6d2t08HW9w8luu1aanlihe5D+RwHkk1SN5pEe9mY+70X9V1ITvMYxViYLPQe2C7sO8ktKn+HyKw89z32+o3T03wqZ3LuEet5uHz0FvEgGwz9xqu14pxO+k1Krcavec82iAQozn/uVwPPmA7Z3KZMSCikRjnoM/gZkbOZMa9jv2GK5nzt+YZ6pozzSUwy3v0/ITVeIjrfpAF1x6bZO46oTc3tzimwfYXJ+bSOElZYfejgzAQOE8WggoDZ64XieNkuoQHZU2aRR+wCFKtvQ1CvcO8Zs+YuZw4qTNNxEHgF91Z7MkKvcI8+58vTqZUqGHAcN0nxvr2DP3enUy7qRikxQ7UiiKFUVla05ocgXokEW9rsMeoYaWYBdRw0qPYL+tlcnJBYpcYSLF8vJ4LLLbh7mcZcxJNFumc0+saTfuSccAUYm59ehBbUXapaW5KTxcu7gNawCLJjuOQ1GoE6lkQIThI1KskqI8qDIi0Wb4VDO9WczMJIrObNpXk6k4DgvZjKEGuNksXYXwpCghkqNXxqWrrphVGdl71dlUUTMckVI79KjaBOGMO8Xp34uSmsuuWtXCaIClXF603Y6AXyrJXaXRTtaRRbLuVSaNLne/MPSsiI8PXv9STE7bhYsDF/8ZKhp6AHrU+V8pLg5YBi6qv1/RdC54nRlSl8f/5deDX8PIx5VNKY2oMzVwcddIsbpjRfHRL+u3wJb6L/dlwqWGXcPbu/HFY0/Vb/9wEM6rX+7zGLLk7SYUfu62bM/rZ1iu1Mf/snzwHyyf/CIp6RefHH3N59nywupdlq3c1K3qhnN9d/3+qDL3oXf/Ly0X//emWOG++tfjwazeVM0/egVp7C7ue1l9flo+eZEMsBbXnoAkJsF/dZSkbmf96qRiFXM6i4254sdMUOQ/kd9wA9fvG9nQ+KVlgO7O72TMNqvzf73u5HXUX+tWObf/5P1byDVEcbu2vb/I49r/3HJQf8Ey9FHOR7Unb8EtbXlFchOqM3Dihe6aY774jBVMsY4Zw+e/cRwzCgqLv3/8KPpbcxlO/y4n//n/gULbSFqj5csBW1f+6Xx1D+5Q5g3Kuy0I/1/HV5Q13cLVLrFyu21dCtxyl7mFXbiDXRn7s3LtLeht2P0HppWvH0v99fZ6xmW4zNrSbesuHZl4k2BRM1qdza/l0nusNpqm29K1m1XoyTv8gUK1uhVRbB25XZVaKxhVYnMZYkEvfktVms4ZyQ8DM/fVWvXlbJYgW/Wu3uwb6bqurQO5lC+WbdXWHUNPF3/DTmm0W2VivlQO1bVXmEtXVNk1Ke1DCBez8nqu6jM3UGYaUY4IKrZp8MOYYfTfmFhbzaOXZca9mpQjND1ks+61S/foACZU14peLWg91orsGD7V4iyR3iOVpb8Oz9kVVXoc88BFW/SE9VADd6aYfBNNO+idtuI9w+rLNP2tXtqm0gyA/Zhas6GFpmmbVW0vUCErUWWzXajp6ae/bc9tqh6LtJCmPppctYpKf7269YthZDbiRH1E8XWlGdUf0vL2KBhEzPY1GpOVduhyViQ8yQAAEABJREFUte+g2sOabSrBMRvSoM/vqj5iBWTz/mQ3qnJLtuozJMZTTOf0Qr57lmhMEf3ShNCjD7EVGGqzKkOd3IbwpM2l5uw9rKqhLvVqgfpc+x5VO91YAFu19AFRUOHmIWGrwx0LsVjzO8qWrXIJnvwtRpjGMfZO0oyY9x5rQHRxPwv4FHfLpbLjqqTg1WMfXb4EXVm+It45gsLv2k0vX6r4u9Lajy6urbbs24iuzFmJfbG80FSC32HTd3cwSXrGiV5bXME9mI7fcAsPf+1EgWHNpryrFXk/rT1+aW3tRRR+wNDZNtJenooetesKm/5g6Lk0rrbvcJXiND3ctI07WPlCwupS5i3ddw1gpEOhuKj4tSIpGgY7OwavHXxhHdKYWvGZ83PTlA/YiUgGWIuVTEASkW5Ubcnv2/ZJzfOx6GSWZWxk7GjxCkbGD7hYXHRSTt4Kw+eDyJEleTnII4DYrL8vvvHbT0wdbT7KliMDlU97a6F3mej4eN7wyB1YnpGz4kNF/v6GDihsO1Ycz7DcezNmjMXy3haGPz7/fy3/zbFL8EXbsYwXcr6PabE/wAbCqpz8OEN3v8Hw25HfKlNxu7/S9PW5nosMS1I8w4OO/+LR5cgwiM35278xGC+CsHjfDw66W7nhayegOwHzuNmDwo4uT7/o9SfEuTnMcARfrszQtJ4PTGorFBeaVDnF1U0m4SG6Kmv6wsPEGS0oM446TpRR1ypEj1Dul2hzYx27IBe9GovypHrmsT7OOGfzBzppHhrw5+YWcJuPolcDsJ5qjSmQuOfVLzhpq31G3+4D6kl1v16GRI2T4DvkMHmS2QV7ZKZC/OgX7NAxTRZQfbNMfkrbeme4/cRwQb7AV38GB6u9aTSHaPyEnbQmh2L6HNwEHgiF2KNoLrUM30jsPJWsR0qh4LRGqfM9f5ZRTHJoHm8lbffFZoDADMr6jtbAJM1sLnMvxfF439gdwBW9xNO8mq16Vwev6tvf5FkD0kV7rbaazYKUMRg47szQ7Jekkk69aUAXJCO1t6ZvH1w4j8fFf02Yy94mFR2V6g+LY/AZzp1DIxuZ41lsAqKL+1mgfzsTlnN/OE2BS7J+ZbmoKX78jx2Kv+LvPj/NWlOz3Rr890fjH4XBwyXVq+rxOyzuskyo0rFb8tV29A77Je6R4Gvfz6q5ePE3rzxu/51i3erdPegp40LdHAtT3TLwja3mR5jLu47bRy9Jyijed9Jy8uXTRftRb8x3aeT4G/kjFd5uhMuZtPO0W5qFtp3dNuUDdiKSAdb6lPgdOONfK3t0X2nTf/pRQ3HyZ0vuoSWx+PRXp9UZ7K9qNiVImpj+xD1qTHZ5+d8WPt58svbEsSzplqBaHbDlMOojor4L6tDYau7xuP5094v927sx8ye7BBN0ThP2CbXudbpS3kV3VW3kmN9O52xCr6Qx4sO09YhccEcnW0UpP7tX9ci4HsMXyQ+coTul7fkqI/Rq6629+4X4SVqktZ/QaMe/s95p1Z6wa4twMBDu77VW1hndLkS7n0LuE7z1y+WFCZOt1urCCkETftce/9z3qx4tkpbote9qm7+Ry1ZPJmd+6daD0gq+FvfGhrQF01EVwxXmlWl76VqOUvHBMLigoJnxcYi2fTfasDmICN72LtpYJVpqVmdwnsNVnIK9XYxGmnaMmt4a66wEqYxILqab5AIn3xtKUbWecRl3EI+vuO4hvOtYs2hPGY6vvir3cRAQXdyyflhY+Q+XyqvRq348f1VPN35TgT4zOnVf9t/eanqRKu9evjbrtX0VL4Nh6jd6/6qTnN0d1Cqr/qQoFMLdu07nn+4iNjy+hHbji8vpzTk6YjDiYfxbH75I/bx7uTCr+J8r8sDQ/Z+QmZVzXNs0gsIM3O371IB/DzZeAj7uKadSqz2/jruL7oZHY70vyGi8puYFVezR+lwPJT4r5+6/NfXcxdVGznf0jOCDe6w+JAOsnaRiUt5PVPVNyyt+UtuHzZ6Ea6bkPwxjY119bce+3pQeH78x6+6xNkb+XcNvGlb8zY9TkGv/5zhDGTm+NaE82CtCz8+pTR/eTcrYovhVWUZnz8WZ2jCef3l+obB6d2NeoTfej/wBGwjX2o7e2pSamPnjvzne9BFDudvX0RkQyP7ra6YpR9r+b0dmxjpwOsCFk8/CHwwG5l4dUzVd2MdqBByJRC/pWtxD5AMazTmZ5GkwTkhqO6x5jqXo4grFb1ZVF4K+J9J/cYbsT95nZh5LgDv9XDa7U1O9QWuj3Y9Cm+kXVzXHxsLL8Ae1pl+YbKgHgApt02ZptJ1MYuPGZuYjk735BY7iM1H2SzoUhDCAX6gSRSjo8pL5xi5mGM18pRfTx1an844Dqwan8Zw7VAVWh7SiMtPPlI7Xpfd6xI7JvdcRj7fG3NWDXbdfNt2znZxO9JDBzM7zereVk8rH98DEDMpTJGn2yjEqWOmaOwJRYZlmp0h/zhSYvNnLyRMIzKax29tGD2PD7G0trRtFKffISO2RMblwD4P4kEOT4zlGO9RAFJuFDmZXHpqsWtL22qz3S2r7k4r3Zv5GRPH561WTfVBZXlj5L1+/vhJ97uWX9CsOvjLlG/1k+tx092hSwuMv/u6pphN4hD3ptcq8U5s4FD+1YSQp5tYwfs4kpT55PD9htyE6p/xXULGSSlid1xObdOM/bi2XVr596/UEZMjqksHSgyUrYEnuwY8TG9at5PP560o6YUk0cJNTR/Y9/eLYV/0Nar3CIn8c10p4vOTGrqM/S3LbAtBR+0+XRn6/HQ+5PMHn7zKAQHV0x2Dh40gB/9m64VhmSMfLPOneg+SNidZOWgEg9vnag8m1JYdwyJyCbdJLbhiRzaggsxHffzSVrFvPj3v26MZG5ZOAHfn7iy/gJlu3+08Hf/PacjclfzUCKvXwmmOqp1GdiWXDzo9Sf/MUdj61gXtEdY/+xMTaE86X5GzJiinIyfR2yQf/rWT9Oj6VeXRTk3IDLNlS+/Hjv16Hda0rMUQvmVAZwKAWrecn8MtH9lW+vByeL6+BigQqgf+TnljBjUF8k3hrBNwk3gtT7q+Mjddnv0sXvN9C7eVx0MfZLL34lFoUDWkT8gFzZbU1tPQRzCK9rtSUzN3DcEozZ30x7W1T2TUp+pBMURzqVav6lJo+phFtlcR4JQoLZMMHar2PsuGWZqtM4vvwESPZKq472OxcrW4vNWXHIVwEtet16mdiGKAEuPez1apulHFBIH9bpNnAoqhE5cRvUQL5IakuI4pFJdfeFsTcpIchsDrAk7kybpr0Ja7XrjnYS/ZU0a9xUFOJ6ofZ95IneLNW2paOrTxkF8QMIysnrcGVtbTy6tKwZM4jvDr+Gc3WmJitmjMr63iPIIh40g7w65l5BKWpdemaNRwEmqCGat8rhtXq9p1WCWoX9FX/AM1e6uFDu5QMUa/FFxBT6MYUKo5DbbPXHsJQq0/tsuZRHNQcaVU0mw1ACdJsyjXP1Q2gv/f0tCPZHPRVf3LhSD6AvTmfpTjFHOKNs9doSxPOHvyHsBDf+ny956spokRvqBzoViRC7PM1Fpq2WC4cbe22vYceKVn1NkxHLID4MQWSXmm0DKDPvZYLTYq103vm4uoTViTNRlvQ+MbAAJKzIZa5HJu573MbIl94r+Yo/XHhckSML+6w2QbwZ5WkV45avqEHLKdr3uum/3UDRCcVNnkMady2lnkmxWZWnGVMs5ytyEQil/yoBlX5+LWxELhEUFivH8A8WG8h/hqRqOjuQrEtqwZr9gysWZif9o7zlOFE5gQtyBcsgbnmRXKitRCIpI+yJKvWcrbUF+cYOdPcINUILwSjuzBmw9ryoxcvoHbsZkBAknyOnP1VDvMNAzfiBQteTu/E2MeXupsb8cYrupgWR21hwDhZLGdrnsctga7doyBjfDgg1rHTkdOn+n6SN/Zr6bVlRy9cxAbuy0CtBDBOF0PxIYOkAPxFXo3hgmWAthxjWgG1+zGLDd0Ip2rqu+hKFBrHFCG//G8SRsBUmxzt6KjDMzQxRLe/yQO2qKoX3Qo0fa2l7Gn0dwtjFNrUkIP//AQlLTTD0t9aJlw6lfiIuBYjkDWabNhgm+1ai4wPokOO9q0xY7atVvdb1QLPOVdupNXopcRzCjFbW0ZbCxA38rqfxkvX3jSMCwaK6f1c0xY8hrnxpwiHg6b7209fHW0UA4i1o1fLVjKXNlZd/Q4B3a99v93h/kgTUB0sulaBLJcRhevMaEXt6PktHFPNd8ov63I4bDRt0p2hMcOYSYB4sJGIn1d2mbETmWQexVY2NrQ7zsi4wCt3OzKOBx0yhb1ZfeaaDUlGwJoOiDAg6EY5gHDGEDEUJNb7/Xxl2VWsHdgbqzDHEE33NogZTxGq3ntJJvC+fiEN3M0F1CndMDrCMLZrD1+lh2wOul3GxyTgy1qGbLh/yTQoQAwarR11nJEnAKCB3O9GbeirPgo0YzcqFj7OHSSEXaAbN3x3p7Xlmkw6rt0Rx4yKf3SZUVXCTBCYJQKG3ajbEVMZtIc0S5Gk2oJDwPrucyxhuwy/mC8432bn0GOyqqc1ZSfss6s901q9+2s5B1TeN4yZ1sb8JLpgFO61PtjXx/VCIsSRzH0DtKU2i+mkIJNQ38hW/zw6mF4Z1y+ZXgXC9UAiwHvzjMPRVeZ+MX8gPZh7o4V7taKmKvx/K3Mv21/iQJ3mEU3D5hh/6szOIi669H1YpO6Ejjc4Jb/DntxlPqHjoxmuhv0Bvy2eoQTCThAgCBAEIguBaIGsVS0KgU0J8tq37qffgk2MsOjSqy45l7fd9x8zN2Y/31emPLNpy2RzyWDPyUoQIAiEDgGiafEhEFHR5VbTP/UU7s3xDpjAfc33FbtFtbFpd8ts+z6L71YgHhMECAIEgTlEIJKiy61Pjt/OyvrhOO88833BYE1qanG535xXIz3+81DhWbw8U4x55zpL2pzz+fE2El7GAUoOCQIEAYJAiBCIpOhiMhjWPj72S2F/BPpG1h7UX7B45ry6e1wxYR6qYHOdrVmX8fvuS/5y5vmMiCcIEAQIAgQBjEAERZfB/r6k5Mn/w+MH3NhogOh4Zs6rwHmogs11Fh0DTrsdu0lWggBBgCBAEAgpAg+FVNvcKQuYh2pJ1tzPdTZ35hJJBAGCwDQRIGwLBYEIii7xiUl9V6c590nmxHmogs515nJCDJsNZCEIEAQIAgSBUCMQQdEFUjIzL301OC0EAuahCjrX2eWL5/469X5TTE7LHsJEECAIEAQIAn4IRFJ0Wf7jLcs6OpiZ57PeY/552zPfF4z7b/N4RcCcV8w8VOCbOOuCd66zvlNtT23JWeLnb+ScEEsIAgQBgsBCRiCSogssL/znDU172piJ1+8b9JHj6s7CfbkkuNw3kkQAQYAgQBCYOQIRFV0AhKr6jGMHO2fuR0ANQ52h8Lhist83B7ATAkGAIPCgIUDsjWwEIiy6AB7gUvlmgrkP7F8+otsAABAASURBVDJ31vhmSbwPMaQqQYAgQBAgCMwGgYiLLrNxgtQhCBAECAIEgQhDAEeXm+FbEBrhUz7HmufNl3vbGUbVgcZFlDGB5gWlIJtvLPSF+LjQWziC/HPfbDi6PBa+BRkRPuVzrDmMvoRRdSCIEWVMoHlBKcjmFQt9IT4u9BaOIP/cNxuOLuiIFIIAQYAgsGARII6FAwESXcKBOtFJECAIEAQWOgIkuiz0Fib+EQQIAgSBcCBAoks4UJ8vnUQuQYAgQBCIFARIdImUliB2EAQIAgSBhYTAxOhy99alpl0v8rNqpzeb5BxDEaB9xLDrWT5e1hd96J4+OZACfR8Wrcc8/Gd3GeZmFpk5c2ukrZjivNExZ/KmJygAxulVI1weBOz6HSkUXhJz680eGt7plXEU5S2sqJTqAUxlVntrASuqSMccuzd+lNaCKN8ibXMzhHc7mY9eq+ytUlaUx1SXWSOhOBxUUpSdTL4kTEnECFE+iIZ1r3tBqzQyTF5RD9aeWDt3CPhHl/6GkuKGkfj42D/OnYbpSwrUfl798ueFBovFcqUx6UB50y2AQMqtpvJ/fLTmCmLq3t5fpD4/fX3zzjnyW+XuT0M+0VkgjPPu6MJS8Jkq2yjrpWl6qEWwX6EZ9nknqhpCRKYYVcKnZdIEzyX7CbnyVIznhNn5U6zWy8Iq66h70eYwHOHdTOqj2yx76zalzuvQ8AcKVUIrbaNt1+RmiUrvAus7uRX8FoQQPaTl7WEgOqXKvik3IxKiHJBrxuKuWyDZLkYE/KNLYnHjyRrFjyZPEDmvEAVoH/zcsCInazlSGr02Nc1gMEEgBUwGw4ubMqMRU+yGtEcNn4el04W0B5SRDuVeVmV1VsCFeSYEwDjP+haaeKtRz5OIucitaGF6hl7fg44mFv1+FVUqwzzoil0n38mqPSRGh54ykWK33RYIvKHIwxPW3dQ+2tvkqpjaOm8UNJ3Tizam4VizTCrNaNFfAF751f79QuyBy2Z34j08HIMZmEMADjvGc0R2ixkB/+iymJGYY9/vduxSsfZXZn1/juXOkTgi5j4QuNNcd0Qqz3M/QZ26HUrWgVrxUp/AQAptHW7O5bDQ4Bgl0ZhdPs6IPLijU/6MVVUj9qXdS9mQ1tqmw0HErm8/N2yl3WbjcULOw9l0jRaH2WfULVwlGj2jHk7X72jAFDcX2S5iBEh0mZfGv/tpeblTte9HIR8Wmxdv7lto527+E3xUdnfet6gIEGB9V03vVYlwdxmcpxQKp7pqszvSYOMCKQAidY+J/sYx+p1N7SpWNI2NteEKkbU6daUKx/6qccESuG9qam/KcOTI0nPSuDzKbTEzTmhugFJ58x2Az1Syy8qrNBo2bBEcKNbcdPNEwLZD6f5Upgz1188I8D3cJpDoMh8tMFi/v2nw0/JU9Eh94zi0lPDzGiJmwG4+/L2XzI37LF9aUNk3F7Nf30vZPF936av3CxSvukfFrHV7NNZTckEcRRU1wxEpJd67byKlzgoxvCeFbBSNotmiDAF92zHPJt6H+IE61QdWXakAPZGlR6C5iMp+1wrRApmOdthoukdGWVOS+WPyY1bK5Dmt7Z2Ahto4+QUCFGSXSaRZen3vGE+Yj7Kq6CEU8+iqOR+iDrNjD4D6iI4u8U9l3mjrQN/ywdVj+H1mZgoEUiAlM/Pj0wY82jBiOHcj86lISOkSr9DbbAP4eWp5bwvk1luOFUeCWQ/A/RgBJvLSRNZWHe5fuIz6DpFoA4DLab/jscx5rE67VV7gGQfjlfWMjqLHLnp+NRbAVi2t21MxkSLnXVElplVb8S1q7+2lqWUsj6zw7Sb1MaHMhBxC7gzR2q1Q0Ei3v8nzmum01is1aXLJUjCWstLfsWK6q7frXFq6EFjLKLq3F/9UzGXu7Y3hcPBFsi5yBCI6usDTqo+easrk8/mrSwZ/UVmIvu8HUpYXVv7L16WrEVNqQ9JHqqcXeYMS9+8bgWfU7WkaIUVRcVLr3lr8CaFDwdlYzTxNzeqdvfJtopnpWK3USrqeQ/0bDk8eXactdPd7ZiZjjrmn8jGIKuu72WhkLLdH1nVYjPonaXv1ueeeQwhxHhGbd2rkjwG3UKtZqhYgUly2qUivfiaIEEJabAgEiy6Jiu6u8GV19NMem1lx1oKXC42vuH/JFkiBpFcaL2Aey9mKzNhIa8Dn623vhaNP7gfj/YCyCOuyRQdM6AsCTfe3lAiw/5sbRnvKmHd4gdrar16NaRPXHO1oo9iPOEZhp5W39COJNhvdWMBFQ2R+fGE5mcJHjz3ixlHfj6d5b7bbbDbTYYnHeHZaWavHIQ9E0VzJYS9o5WlsjwyyW9QIBIsuixoQ4jxBgCBAECAIzAECJLrMAYhEBEGAIEAQmBSBxXqBRJfF2vLEb4IAQYAgMJ8I4OgSNBdsaIjItdAoCoGWMPoSRtWBwEaUMYHmBaUgmyMobez8mLI4fPzLGze+R0rYEQBADXEDR5eguWBDQ0R3fGgUhUBLGH0Jo+pxwHoOI8oYj0332iGbVyz0ZdH46FqxgpQwI+C+2XB0QUekEAQIAgQBggBBYA4RINFlDsEkoggCBAGCwH0gsLCqkuiysNqTeEMQIAgQBCIDARJdIqMdiBUEAYIAQWBhIUCiy8JqzwfDG2IlQYAgsPAR8I8uIz21hev5eFlf9GFf6L2/e+tS064X+Vm13hmFRwy7nsXm8H32BFKg78Mit9HP7jKMhN7o4BpvdfzUa3lNT4itCoAxuImEOgkCdv2OFAovibn15nE8OKMJFUe5CysqpXoAwG6sliRiXsrDrN/hYcBsiOkdK7jMGgnF4aCSouzEMz2Okxmuw8l89Npjb5WyoqRtzGlQ+51mTVEKi8VReme2by2I8i2eikxtslm0CPhFl579L5xM+xhP2XWlPv6XJbU3QgtLYMrewDzHgZTIzHz8qTr/P4q7EZQIyUPKplAiGQhjaJvxgdc2aVZgJqMJM38wbVS5Mx8b94haMs7QaBnS8vZIUbwRHcDzvTOzvveq1qfJ8niBmYPDD9GkPrpNs0+d+RhgWCMW6Ta22Bw278z2kZfd2e1KyLfSKNCFQOmDoMIvumz4V/r0Nmaq+Oj4pCTn3buh9SCxuPFkjeJH7tkqserBzw0rcrKWo8PotalpBoMJAilgMhhe3JSJZwaM3ZD2qOFzb7cH1QpjWbokZkw7O3bcyRh5no4CYJwnPQtVrNWo50nEeB7jaGF6hl7fE8RR/X4VVYpnT06rcXS9xcxvGc0TCJxOnMHRy9+pVlEK2WNgOqcXbUzDt8AyqTSjRX/ByxC+/dQ+2tvkqpjauhyPfUHsv66tdqjqXuVhpzxcdtttgSDBcxIROxdoJEBRwOGA9Ai2qHoNSF8DKg6ivgfKTkCLtRKSC+C5OOB8D5J3AO5XTlIryh0zboJ0FZZACUFvB3f1XCFQLKBeB/sAZMdBM4A0DvRAFvCLLmN4XGtquFO4ZdUYgRzNDIGnVY2P7k5I4PPjNhm21eDcATOrT7gjGAG/zMdeOy2aum9lBWOZtZzNB7XS7QXo+TtJ5mBvxUjbTyfz8WWTObpVigYJ0Xjf6zo7diHisjvbj4FmA9A02KxgLwAdNhLsGUAPwehl0InByFDMdmgZApsDpB2g+gyC1rIKYXQUxACqDEhuwxLMpZC9B9dH1TU9QH8LuU3QEgPtQ1AAoB0CEb642Ndg0WWkoyTnZN6vwzcJ/wJolPPq7Ve3d39lsVxpTDpU2vSfC8Cl+3Bh9pmP70PpvFW1vqum96pEuLvs1WHXSTe2SJvcs/QzxOt1alqt2oiPuW9qam/K0HOYytJz0rg8ChMjdXXqShWO/VViT3o0bGag/dbrveAU11pp2matHs6WtyE2kbrHRH/jGP3OpnYVK5pw9jVEDWNh54Gsh+lnrPGEFmQMexnaAPBBmgBd15njZcAGgGiQFIDeCEFr8dx9sgHQDQAaCUS9H8FOcBphCFVE1dGdEA28leBLMYfIpCAEHkKrXxnpKE2v5J84rSAdFz9cZnaCRvBi/3ZL0hKAH+Tk/7XBcGlm1Rca98Z9li8tqOxjnrYPtncuffV+geJVPHLmccSuK16jTu7oKuN7CGinP6AWlMo8TNECmY522Gi6R0ZZU5LHsSHOyCoDdaoPrLpSARVHodGk5iIq+10rBNhPPcbjZokE6KkK7LQMQa/FChDDe1LIjgaIZosyBPRtR9j9sh6ECj7uZ9BMf2Ka9kxVywVOPnQNMTJpGO2BuGkKXaxsD/k57uqrfWm7o/pYmQA9F/2uhOUk/kHNfAysH3C/vnQJ/1TM1XfpUgybDWR5UBCYNCsw44B/5mMAl7k6S+Y4pNu1Go2BMRxoE3ToDMYyByOW8JZJfZxe5uOYDEnyBxo9/shkN56jRU/yIPKyOzud4MDZpsF53jMIhjC330YbAAtohyF9JXN8G/DIngtam0GcAU5nkFrgXlaCxAma88zJbdC5D5iziN6Ezzi/6DJ4uGR378i5slT+E3xcdhnCZxij+WnVR081ZfL5/NUlg7+oxF8vAinLCyv/5evS1YgptSHpI9XTTMVwb5a/XH9waVUqMmp1/sW/OxkhVoUblQdE/zPq9jSNkKKoOKl1by3ugHQoOBur0fs5gFm9s1e+bWxQ3XpQqvzCrt+GX/bR+z61A3/NNb+t6t0uHz90Zn03G42M5fbIug6Lx0Wh8AEylY9BrJpo/2My7QGblOJQHF7Vyhb1RoDVSq2k6zn8JYYnj67TFnq6bUFkhYokeBOkbRDFguRDIIgBmhmrsx6GRB6whCBuhTS3JUMg5QH1MGizAP2dBq3lZkRb9Smw5gEnDqg0oCd5ZUx/GrI55Ks+Qgv8okv8trM2G41GMDylIhOzhHhNVHR3+T75xGZWnLXg5ULjK+7fkgVSIOmVxguYx3K2IjM2xNZOpi56ec6vvJaXbgiDVX4wTmYloQdFYIqswBMzH/PeMo2OOpjfHzM/RD6AA49gb3//XsF40bwJmYPHXwvP8RQ+egyaKvMxAHerlkYPC5vNdEDEPGYjL7szG6rMMOqA/kZodwB+SwAQ7oR+KzgcUIUiottRIbRbgXbA1QOAHQmoVXYZfBmggc/8BAANjl0DGR945TDaCO4FsZUxnSG5EUZtIHJTF/fWL7osbiiI9w80AsR4ggBBILIQINElstqDWEMQIAjMEwKoezHWC2F0jO98MASymUsESHSZSzSJLIIAQYAg8KAiMNd24+hyM3wLcid8yudYcxh9CaPqQBAjyphA84JSkM03FvqyaHyMvnGDlDAj4L7ZcHS5V1rYebyOjJhH6aEVHUZfwqg6EOOIMibQvKAUZPNCT3y8YtH4GOakvytI3uUV+Jfg6A8KRxd025FCEFg0CBBHCQIEgVAgQKJLKFAmOggCBAGCwGJDgESXxdbixF+CAEGAIHB/CEyvNoku08OJcBEECAIEAYLATBAg0WUmaBFeggB1njQQAAAHY0lEQVRBgCBAEJgeAv7RxdXXFFmZj6GtmONbSn6HfHpQMh8jU1EZaSumOG90oKNQFpL5+P7Q9mUFTsytN48TpVfGUXgyMWbLikqpHsAXncO9mh3PUWnuicgwBZxmTVEKi8VRulvebqyWJFJ4mSCQYQ7PZjIfvdbYW6WsKGkbc+oyayQUh4NKirITT/kIMKx7PQU7RCXmVhoZUiCFqUs2ixgBv+gyeKiwelUjnrLrSn38L8ubboUWmP6GkuKGkfj42D/69A4OXl2776LNvdQ/D3Be/fLnhQZk4pXGpAOMhbeayv/x0ZoriNS9vb9IHUkTl478Vrn705DPNh0ERh+e5GAaCHymyjbKemmaHmoR7FdomNkPmWqiqiFEZIpRJXxaJk0AuF4nLaiz83icOwwL3gxrxCLdxhabw1aVhc+Ne0QtGWeQPHpIy9sjdcckfCGM66Q+um2yt25T6rzTbQ5/oFAltNI22nZNbpao9C6AU6rsm3Izcgl5dECuQVE2kOKWtPi20ijQLT6vg3rsF13iS7sv7FmL+f5kH3HifUjXxImZjwFG7P+V9Dj+pb7HkMGAXMgQmZmPkb0jHcq9rMpq5gGDTkNWgsAYMt0LQdHUWYHdHvoyH8NKeYu+oWzzuDkrA7ICT5Ud2S0u5Nupfbx35uOHY7yhB5nOYaOTQAq6Et7iIpmPw9sAgKOLvwmG3U/wEx7LH/7Xejzjvf+1kJ99Pfj18aIE1CXn8Aub+tBLU8gtmK3Cux27VKz9lVnfn60AUi9iEQievsVr7mWTObpVikbP0FjS6zpm1Mh7yaKp+1ZWEMnZw5Cld3TKn7GqasR4wmB0CpCyIa21TYffNu369nPDVhrgGXULV4n8ox5O1+9okHGDUZi6YdzYj4FmA9A02KxgL/D0J+wZOPfX6GXQiT1JX8x2ZtpjB0g7QPUZBK1lFcLoKIgBVBmQ3IYlmEshew92DlXX9AD9LeQ2QUsMtA9BAYB2CMgcyQidwOiSue9Ly1fdNfBz5fE/IobwlkyV/qzlOm37ZmCXq7T8oxAP1c3e97uflpc7Vft+FPJhsdmbPJ81O3fjdEFP8Hd3zqeWUMkOkvl4nOpgWYGZy4HZkRlyhG2c08l8DJ+pZJeVV/HIWIvgQLHmJgShhNuxoDmM2csYs/ggJZmPGSTmdRMYXbC6JYmFJc+3fXIOH4d1XRIvXBuL06nGZqYlDf8h/OlUp4fGYP3+psFPy1Of4PPfOA4tJfy8hsHp1VyYXAs78zFus7GVeozHzRIJ0E0L7LQMQa+FyTpm1xWvUSd3dJXxxzgj8WigTvWBVVeKk6FJj0BzEZX9rhWiBTId7bDRdI+MsqYk8wENrHHyCwRoQGyZRJql1/cGoYTdO+tBqODjfgbN9Cemac9UtVzg5EPXECOThtEeiJum0MXK5hdden5ObTrEPAZdl7qNG1KZTzDhRMasXp9VO4gHxEYuXfqa+wNW/FOZN9o6cBfG1WP4fWZmCuq3Z2Z+fNrA8BjO3ch8Kj6cBnt0xyv0NtuABSdhe28L5NZbjhVHglke68huSgR4aSJrqw5/y3cZ9R0i0QYAl9N+x1PHeaxOu1VesNRzGriLyZAkf6DRMwNJxnO06EkeuMzVWTLHId2u1eh5HFgjDJRJfUwoM42O2obwLxe0W6GgkW5/k+e1z2mtV2rS5JKlwFpG0b29eNDPZe7tjeFwglC8tcK2dzrBgR8L4DzvGQRDpthvow2ABbTDkM4k+4LbwDgCrc0gzgCnM0gtcC8rQeIEjft3Q7dB5z5wXyLbYAj4RZcNO0++YHyRz+cnrMzv21Fb/MNgNUJJE2yvz/n8xdV8fsK68uiq+peXwwOS+TiUIBFdc4zAVFmBJ2Y+DqI6ICtw0OzIQSqGkjSVj0HsmJD5mFuo1SxVCyiKiss2FenVz0AgJYiU0JKC5jAmmY9D2Qh+0QViNyiacBLhgQFLoyfTsJ8xoThJVHR3Kbxv+rEbShs9Br23ZTkebYjNrDhrwcsFn4VJrzA8FsvZiszYUJg4Ex3P19veC/nPxpCBfjCic1KmjwBbdMCEvinQdH9LiQBX29ww2lPGvMML1NZ+9WpM81tXll297GbAZO5WLW2zoWI6IELfxnlvmUZHHfQQ7hDg7QERZgrzOoWPHsvEjaO+XFu8N9ttNpvpsISL/wYBormSw16IytOQj0EoHjHh27GhygyjDuhvhHYH4J8eAAh3Qr8VHA6o2ug1TAjtVqAdcPUAYEcCavnlHOMzPwFAg2PXQMaH8cnHEFsZ0xmSG2HURr7qA1oeQispBAGCAEGAIEAQmFsESHSZWzyJNILA1AiQq2FDAHUvfL0xtxHjOx9uCtnOIQIkuswhmEQUQYAgQBAgCHgQwNElaC7Y0BCRFaFRFAItYfQljKoDgY0oYwLNC0pBNi/0xMc3Fo2PYU76S/IuIwTcNxuOLqvnYZmmyD//8z+fJmfks4XRlzCqDmyXiDIm0LygFGTzioW+oD/4he7iCoB/X7HiO1LCjgDTECv+PwAAAP//4msoIwAAAAZJREFUAwA6XoEb5ysQ9wAAAABJRU5ErkJggg==)"
      ],
      "metadata": {
        "id": "IMdQa9lRMBJf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDEExiAk4fLb"
      },
      "source": [
        "# Fine-tune Gemma in Keras using LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro"
      ],
      "metadata": {
        "id": "I_1BItm1CcZ8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWzQEqNosrS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/core/lora_tuning\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/core/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/google/generative-ai-docs/main/site/en/gemma/docs/core/lora_tuning.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/core/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSGRSsRPgkzK"
      },
      "source": [
        "Generative artificial intelligent (AI) models like Gemma are effective at a variety of tasks. You can further fine-tune Gemma models with domain-specific data to perform tasks such as sentiment analysis. However, full fine-tuning of generative models by updating billions of parameters is resource intensive, requiring specialized hardware, such as GPUs, processing time, and memory to load the model parameters.\n",
        "\n",
        "[Low Rank Adaptation](https://arxiv.org/abs/2106.09685) (LoRA) is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights of the model and inserting a smaller number of new weights into the model. This technique makes training with LoRA much faster and more memory-efficient, and produces smaller model weights (a few hundred MBs), all while maintaining the quality of the model outputs. This tutorial walks you through using Keras to perform LoRA fine-tuning on a Gemma model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyhHCMfoRZ_v"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on [kaggle.com](https://kaggle.com).\n",
        "* Select a Colab runtime with sufficient resources to tune\n",
        "  the Gemma model you want to run. [Learn more](https://ai.google.dev/gemma/docs/core#sizes).\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ5Qo0fxRZ1V"
      },
      "source": [
        "### Select a Colab runtime\n",
        "\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select &#9662; (**Additional connection options**).\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsPC0HRkJl0K"
      },
      "source": [
        "### Configure your API key\n",
        "\n",
        "To use Gemma, you must provide your Kaggle username and a Kaggle API key.\n",
        "\n",
        "To generate a Kaggle API key, go to the **Account** tab of your Kaggle user profile and select **Create New Token**. This triggers the download of a `kaggle.json` file containing your API credentials.\n",
        "\n",
        "In Colab, select **Secrets** (🔑) in the left pane and add your Kaggle username and Kaggle API key. Store your username under the name `KAGGLE_USERNAME` and your API key under the name `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOF6Yo-wUEC"
      },
      "source": [
        "### Set environment variables\n",
        "\n",
        "Set environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0_EdOg9DPK6Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEUAKJW1QkQ"
      },
      "source": [
        "### Install Keras python packages\n",
        "\n",
        "Install the Keras and KerasHub Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1eeBtYqJsZPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5ca8fb-9597-4d1d-8149-dacb561aabe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/947.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m696.3/947.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m947.9/947.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.21.1 requires keras-hub==0.21.1, but you have keras-hub 0.22.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.21.1 requires keras-hub==0.21.1, but you have keras-hub 0.22.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-hub\n",
        "!pip install  -q -U keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGLS-l5TxIR4"
      },
      "source": [
        "### Select a backend\n",
        "\n",
        "Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, you can run workflows on one of three backends: TensorFlow, JAX, or PyTorch. For this tutorial, configure the backend for JAX as it typically provides the better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yn5uy8X8sdD0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZs8XXqUKRmi"
      },
      "source": [
        "### Import packages\n",
        "\n",
        "Import the Python packages needed for this tutorial, including Keras and KerasHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FYHyPUA9hKTf"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RCE3fdGhDE5"
      },
      "source": [
        "## Load model\n",
        "\n",
        "Keras provides implementations of Gemma and many other popular [model architectures](https://keras.io/keras_hub/api/models/). Use the `Gemma3CausalLM.from_preset()` method to configure an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vz5zLEyLstfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "aaebda43-f80b-4b2b-fa4f-f52827efa76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_1b/3/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 966/966 [00:00<00:00, 1.75MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_1b/3/download/task.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.23k/3.23k [00:00<00:00, 5.89MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │     \u001b[38;5;34m999,885,952\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl4lvPy5zA26"
      },
      "source": [
        "The `Gemma3CausalLM.from_preset()` method instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma#_xxxxxxx\"` specifies a preset version and parameter size for Gemma. You can find the code strings for Gemma models in their **Model Variation** listings on [Kaggle](https://www.kaggle.com/models/keras/gemma3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_L6A5J-1QgC"
      },
      "source": [
        "## Inference before fine tuning\n",
        "\n",
        "Once you have downloaded and configured a Gemma model, you can query it with various prompts to see how it responds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVLXadptyo34"
      },
      "source": [
        "### Europe trip prompt\n",
        "\n",
        "Query the model for suggestions on what to do on a trip to Europe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwQz3xxxKciD",
        "outputId": "1ee5899c-b6ee-47c8-920d-a177480776e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What should I do on a trip to Europe?\n",
            "\n",
            "Response:\n",
            "Europe offers a huge and diverse range of experiences! To help you plan the perfect trip, here's a breakdown of suggestions based on different interests and budgets:\n",
            "\n",
            "**1. Popular Destinations (Good for First-Timers):**\n",
            "\n",
            "*   **Paris:** Iconic landmarks like the Eiffel Tower, Louvre Museum, and Notre Dame Cathedral. Romantic atmosphere, world-class cuisine.\n",
            "*   **Rome:** Ancient history, stunning architecture (Colosseum, Roman Forum), delicious pasta and pizza.\n",
            "*   **London:** Royal history, vibrant theater scene, museums galore (British Museum, National Gallery).\n",
            "*   **Barcelona:** Unique architecture (Gaudí's Sagrada Familia), beaches, lively nightlife.\n",
            "*   **Amsterdam:** Canals, museums (Rijksmuseum, Van Gogh Museum), charming architecture, cycling culture.\n",
            "\n",
            "**2. Budget-Friendly Options:**\n",
            "\n",
            "*   **Portugal:** Affordable cost of living, beautiful coastline, delicious seafood, charming towns.\n",
            "*   **Czech Republic:** Historic cities, beer gardens, affordable accommodation.\n",
            "*   **Hungary:** Thermal baths, vibrant culture, delicious food (goulash).\n",
            "*   **Poland\n"
          ]
        }
      ],
      "source": [
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_hub.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### --> Rough Play_**Sampler**\n",
        "\n",
        "seed is same = response exactly same"
      ],
      "metadata": {
        "id": "4h4fy-3003tI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "outputId": "ee59591a-d515-4684-8f6c-432109a41def",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mSnw1k972dnZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What should I do on a trip to Europe?\n",
            "\n",
            "Response:\n",
            "Europe offers a huge and diverse range of experiences! To help you plan the perfect trip, here's a breakdown of suggestions based on different interests and budgets:\n",
            "\n",
            "**1. Popular Destinations (Good for First-Timers):**\n",
            "\n",
            "*   **Paris:** Iconic landmarks like the Eiffel Tower, Louvre Museum, and Notre Dame Cathedral. Romantic atmosphere, world-class cuisine.\n",
            "*   **Rome:** Ancient history, stunning architecture (Colosseum, Roman Forum), delicious pasta and pizza.\n",
            "*   **London:** Royal history, vibrant theater scene, museums galore (British Museum, National Gallery).\n",
            "*   **Barcelona:** Unique architecture (Gaudí's Sagrada Familia), beaches, lively nightlife.\n",
            "*   **Amsterdam:** Canals, museums (Rijksmuseum, Van Gogh Museum), charming architecture, cycling culture.\n",
            "\n",
            "**2. Budget-Friendly Options:**\n",
            "\n",
            "*   **Portugal:** Affordable cost of living, beautiful coastline, delicious seafood, charming towns.\n",
            "*   **Czech Republic:** Historic cities, beer gardens, affordable accommodation.\n",
            "*   **Hungary:** Thermal baths, vibrant culture, delicious food (goulash).\n",
            "*   **Poland\n"
          ]
        }
      ],
      "source": [
        "#seed = 2 (Matches exactly the above response of seed 2)\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_hub.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "outputId": "c7420e69-3988-41ef-874c-5fa438908f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc1Piupm20hy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What should I do on a trip to Europe?\n",
            "\n",
            "Response:\n",
            "A Europe trip can be incredibly rewarding, offering a wide array of experiences. Here’s a breakdown of things you could do, categorized by interest and budget:\n",
            "\n",
            "**1. Classic European Experiences:**\n",
            "\n",
            "*   **Paris:** Explore the iconic landmarks (Eiffel Tower, Louvre, Notre Dame), stroll along the Seine, indulge in French cuisine, and enjoy a romantic atmosphere.\n",
            "*   **Rome:** Immerse yourself in ancient history (Colosseum, Roman Forum), visit Vatican City, and savor the flavors of Italian cuisine.\n",
            "*   **London:** Experience royal history at Buckingham Palace, browse the shops in Covent Garden, see a West End show, and enjoy diverse culinary options.\n",
            "*   **Barcelona:** Enjoy the vibrant architecture of Gaudí (Sagrada Familia, Park Güell), relax on the beach, and experience the nightlife.\n",
            "\n",
            "\n",
            "\n",
            "**2.  Budget-Friendly Options:**\n",
            "\n",
            "*   **Free Walking Tours:** Many cities offer free walking tours – a fantastic way to get an overview of the city and learn about its history.\n",
            "*   **Picnics:** Pack a picnic lunch and enjoy it in parks or by the river.\n",
            "*   **Street\n"
          ]
        }
      ],
      "source": [
        "#Seed = 5\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_hub.samplers.TopKSampler(k=5, seed=5)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "outputId": "77b8d442-d7ff-424e-c573-b097c8828863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNsitWvT084d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What should I do on a trip to Europe?\n",
            "\n",
            "Response:\n",
            "A Europe trip can be incredibly rewarding, offering a wide array of experiences. Here’s a breakdown of things you could do, categorized by interest and budget:\n",
            "\n",
            "**1. Classic European Experiences:**\n",
            "\n",
            "*   **Paris:** Explore the iconic landmarks (Eiffel Tower, Louvre, Notre Dame), stroll along the Seine, indulge in French cuisine, and enjoy a romantic atmosphere.\n",
            "*   **Rome:** Immerse yourself in ancient history (Colosseum, Roman Forum), visit Vatican City, and savor the flavors of Italian cuisine.\n",
            "*   **London:** Experience royal history at Buckingham Palace, browse the shops in Covent Garden, see a West End show, and enjoy diverse culinary options.\n",
            "*   **Barcelona:** Enjoy the vibrant architecture of Gaudí (Sagrada Familia, Park Güell), relax on the beach, and experience the nightlife.\n",
            "\n",
            "\n",
            "\n",
            "**2.  Budget-Friendly Options:**\n",
            "\n",
            "*   **Free Walking Tours:** Many cities offer free walking tours – a fantastic way to get an overview of the city and learn about its history.\n",
            "*   **Picnics:** Pack a picnic lunch and enjoy it in parks or by the river.\n",
            "*   **Street\n"
          ]
        }
      ],
      "source": [
        "#seed=5\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_hub.samplers.TopKSampler(k=5, seed=5)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AePQUIs2h-Ks"
      },
      "source": [
        "The model responds with generic tips on how to plan a trip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ74Zz_S0iVv"
      },
      "source": [
        "### Photosynthesis prompt\n",
        "\n",
        "Prompt the model to explain photosynthesis in terms simple enough for a 5 year old child to understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lorJMbsusgoo",
        "outputId": "ee4679ad-240d-4126-8fdc-0649f891fb25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Explain the process of photosynthesis in a way that a child could understand.\n",
            "\n",
            "Response:\n",
            "Imagine you're a tiny plant!  You're sitting in the sunshine, and you're hungry. You can't eat like we do, so you have to make your own food!  \n",
            "\n",
            "Photosynthesis is how plants do this. It's like a special recipe that they use to turn sunlight, water, and air into sugary food!\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "1. **Sunlight:** Plants love the sunlight! It's like giving them a big, warm hug.\n",
            "2. **Water:** Plants drink water with their roots, like you drink juice!\n",
            "3. **Air:** Plants breathe in air, just like we do! They take in a special part of the air called carbon dioxide.\n",
            "\n",
            "Plants use these ingredients together to make their yummy food, a kind of sugar. This sugar helps them grow big and strong and gives them energy to live!\n",
            "\n",
            "So, photosynthesis is basically a plant's way of making food using sunlight, water, and air!\n",
            "\n",
            "Does that make sense?**\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "*   **Simple Language:** The response uses simple, easy-to-understand\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "3821ac4c-4d61-45fd-e8c4-5cd3dbbb0b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SMG343Z4lwi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Explain the process of photosynthesis in a way that a child could understand.\n",
            "\n",
            "Response:\n",
            "Okay, imagine plants are like little chefs! They make their own food using a special recipe called photosynthesis.\n",
            "\n",
            "Here’s how it works:\n",
            "\n",
            "1. **Gathering Ingredients:** Plants need a few things to cook their food:\n",
            "   * **Sunshine:** They soak up the sunshine with their leaves like a really big plate!\n",
            "   * **Water:** They drink water through their\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBQieduRizZf"
      },
      "source": [
        "The model response contains words that might not be easy to understand for a child such as chlorophyll."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt7Nr6a7tItO"
      },
      "source": [
        "## LoRA fine-tuning\n",
        "\n",
        "This section shows you how to do fine-tuning using the Low Rank Adaptation (LoRA) tuning technique. This approach allows you to change the behavior of Gemma models using fewer compute resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T7xe_jzslv4"
      },
      "source": [
        "### Load dataset\n",
        "\n",
        "Prepare a dataset for tuning by downloading an existing data set and formatting if for use with the the Keras `fit()` fine-tuning method. This tutorial uses the [Databricks Dolly 15k dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) for fine-tuning. The dataset contains 15,000 high-quality human-generated prompt and response pairs specifically designed for tuning generative models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xRaNCPUXKoa7",
        "outputId": "823408b2-96be-434e-ff23-a86f0f00c48e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-19 16:55:11--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.23, 18.164.174.118, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/64358e2179c45fcf1ada09f4/63c4dabe683d7254493568d2d3995c0e51abc8528ef3b4936497c538cb501e93?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250919T165511Z&X-Amz-Expires=3600&X-Amz-Signature=54511f4579a4780a29053312a1d85a69f24d4e5d025db99e72616bf21805be41&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&x-id=GetObject&Expires=1758304511&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1ODMwNDUxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDM1OGUyMTc5YzQ1ZmNmMWFkYTA5ZjQvNjNjNGRhYmU2ODNkNzI1NDQ5MzU2OGQyZDM5OTVjMGU1MWFiYzg1MjhlZjNiNDkzNjQ5N2M1MzhjYjUwMWU5MyoifV19&Signature=CtW5GdaqtETAFS-Hwy0jzjiWqSfF9JcybLLCZ9wl9aQujx2j64MTpBvzs3p%7ETob-z4A6PYSJabjGIrlid8nxN%7EBWPXMsRUvX1jyeLx7a8wxB548DtnPvVLQ35KE0SEHIXbKlykHcjry4hdy19PiFKjiLv0N7rCOewtPGR6zEGsyCjFF64o8DAOdmjnJ8UQhndpSIlYoxpvpqVER5%7Eyb%7Ep9eVcnQGxHqecv%7EMVyQsfxi%7Em3TkaqIUDVPdwm0ulxSns6hWBqIiIjv5bjjhNPeKJi4hgDrZ0LHFK8cDJk6ArakL4gYVg5fsJpy%7EIoXA8yxRYjamRzdUQOu7pWPuZea3qA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-09-19 16:55:11--  https://cas-bridge.xethub.hf.co/xet-bridge-us/64358e2179c45fcf1ada09f4/63c4dabe683d7254493568d2d3995c0e51abc8528ef3b4936497c538cb501e93?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250919T165511Z&X-Amz-Expires=3600&X-Amz-Signature=54511f4579a4780a29053312a1d85a69f24d4e5d025db99e72616bf21805be41&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&x-id=GetObject&Expires=1758304511&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1ODMwNDUxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDM1OGUyMTc5YzQ1ZmNmMWFkYTA5ZjQvNjNjNGRhYmU2ODNkNzI1NDQ5MzU2OGQyZDM5OTVjMGU1MWFiYzg1MjhlZjNiNDkzNjQ5N2M1MzhjYjUwMWU5MyoifV19&Signature=CtW5GdaqtETAFS-Hwy0jzjiWqSfF9JcybLLCZ9wl9aQujx2j64MTpBvzs3p%7ETob-z4A6PYSJabjGIrlid8nxN%7EBWPXMsRUvX1jyeLx7a8wxB548DtnPvVLQ35KE0SEHIXbKlykHcjry4hdy19PiFKjiLv0N7rCOewtPGR6zEGsyCjFF64o8DAOdmjnJ8UQhndpSIlYoxpvpqVER5%7Eyb%7Ep9eVcnQGxHqecv%7EMVyQsfxi%7Em3TkaqIUDVPdwm0ulxSns6hWBqIiIjv5bjjhNPeKJi4hgDrZ0LHFK8cDJk6ArakL4gYVg5fsJpy%7EIoXA8yxRYjamRzdUQOu7pWPuZea3qA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.164.174.4, 18.164.174.110, 18.164.174.68, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.164.174.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13085339 (12M)\n",
            "Saving to: ‘databricks-dolly-15k.jsonl’\n",
            "\n",
            "databricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-09-19 16:55:11 (129 MB/s) - ‘databricks-dolly-15k.jsonl’ saved [13085339/13085339]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45UpBDfBgf0I"
      },
      "source": [
        "### Format tuning data\n",
        "\n",
        "Format the downloaded data for use with the Keras `fit()` method. The following code extracts a subset of the training examples to execute the notebook faster. **Consider using more training data for higher quality fine-tuning.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZiS-KU9osh_N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "prompts = []\n",
        "responses = []\n",
        "line_count = 0\n",
        "\n",
        "with open(\"databricks-dolly-15k.jsonl\") as file:\n",
        "    for line in file:\n",
        "        if line_count >= 1000:\n",
        "            break  # Limit the training examples, to reduce execution time.\n",
        "\n",
        "        examples = json.loads(line)\n",
        "        # Filter out examples with context, to keep it simple.\n",
        "        if examples[\"context\"]:\n",
        "            continue\n",
        "        # Format data into prompts and response lists.\n",
        "        prompts.append(examples[\"instruction\"])\n",
        "        responses.append(examples[\"response\"])\n",
        "\n",
        "        line_count += 1\n",
        "\n",
        "data = {\n",
        "    \"prompts\": prompts,\n",
        "    \"responses\": responses\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBLW5hiGj31i"
      },
      "source": [
        "### Configure LoRA tuning\n",
        "\n",
        "Activate LoRA tuning using the Keras `model.backbone.enable_lora()` method, including a LoRA rank value. The *LoRA rank* determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments. A higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation.\n",
        "\n",
        "This example uses a LoRA rank of 4. In practice, begin with a relatively small rank (such as 4, 8, 16). This setting is computationally efficient for experimentation. Train your model with this rank and evaluate the performance improvement on your task. Gradually increase the rank in subsequent trials and see if that further boosts performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RCucu6oHz53G"
      },
      "outputs": [],
      "source": [
        "# Enable LoRA for the model and set the LoRA rank.\n",
        "gemma_lm.backbone.enable_lora(rank=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlMLp_NVbRoQ"
      },
      "source": [
        "Check the model summary after setting the LoRA rank. Notice that enabling LoRA reduces the number of trainable parameters significantly compared to the total number of parameters in the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KqYyS0gm6pNy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "a89d15a0-8e0f-4f58-aec4-d11f9608a4bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │   \u001b[38;5;34m1,000,538,240\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,538,240</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,538,240\u001b[0m (3.73 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,538,240</span> (3.73 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m300,544\u001b[0m (1.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">300,544</span> (1.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,000,237,696\u001b[0m (3.73 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,237,696</span> (3.73 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQQ47kcdpbZ9"
      },
      "source": [
        "Configure the rest of the fine-tuning settings, including the preprocessor settings, optimizer, number of tuning epochs, and batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "p9sBNH8SAjgB"
      },
      "outputs": [],
      "source": [
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA0ozGC66tk1"
      },
      "source": [
        "### Run the fine-tune process\n",
        "\n",
        "Run the fine-tuning process using the `fit()` method. This process can take several minutes depending on your compute resources, data size, and number of epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_Peq7TnLtHse",
        "outputId": "d5fc8358-6b17-4e09-9c07-8d15a86376da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 322ms/step - loss: 0.7445 - sparse_categorical_accuracy: 0.4938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fe12650d3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx3m8f1dB7nk"
      },
      "source": [
        "#### Mixed precision fine-tuning on NVIDIA GPUs\n",
        "\n",
        "Full precision is recommended for fine-tuning. When fine-tuning on NVIDIA GPUs, you can use mixed precision (`keras.mixed_precision.set_global_policy('mixed_bfloat16')`) to speed up training with minimal effect on training quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0lHxEDX03gp"
      },
      "outputs": [],
      "source": [
        "# Uncomment the line below if you want to enable mixed precision training on GPUs\n",
        "# keras.mixed_precision.set_global_policy('mixed_bfloat16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yd-1cNw1dTn"
      },
      "source": [
        "## Inference after fine-tuning\n",
        "\n",
        "After fine-tuning, you should see changes in the responses when the tuned model is given the same prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H55JYJ1a1Kos"
      },
      "source": [
        "### Europe trip prompt\n",
        "\n",
        "Try the Europe trip prompt from earlier and note the differences in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Y7cDJHy8WfCB",
        "outputId": "93c37ad2-0e25-4681-943e-00092eb208fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What should I do on a trip to Europe?\n",
            "\n",
            "Response:\n",
            "Europe is a continent that offers a variety of travel options, from historical sites and cities to beaches and natural landscapes.  \n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_hub.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXP6gg2mjs6u"
      },
      "source": [
        "The model now provides a shorter response to a question about visiting Europe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7nVd8Mi1Yta"
      },
      "source": [
        "### Photosynthesis prompt\n",
        "\n",
        "Try the photosynthesis explanation prompt from earlier and note the differences in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "X-2sYl2jqwl7",
        "outputId": "ad0025c4-9d5a-4dd4-819f-e1b570d95b56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Explain the process of photosynthesis in a way that a child could understand.\n",
            "\n",
            "Response:\n",
            "Imagine that plants are little chefs!They need to make their own food, but they don't need a stove or a refrigerator. They use the sun to cook. They take in air, water, and sunlight and they make their own yummy food called sugar, and they give off fresh air for us to breathe. It’s called photosynthesis. Photosynthesis is the name of the process that plants use to make food, and it is essential for all life on Earth.\n",
            "It is a process that allows plants to make their own food, using sunlight, water, and carbon dioxide.\n",
            "It is the process where plants take up water through their roots and carbon dioxide from the air through their leaves, and convert it into food.\n",
            "It is the most important process in the world, as it provides us with the oxygen that we need to breathe.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCmAmqrvkEhc"
      },
      "source": [
        "The model now explains photosynthesis in simpler terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8kFG12l0mVe"
      },
      "source": [
        "## Improving fine-tune results\n",
        "\n",
        "For demonstration purposes, this tutorial fine-tunes the model on a small subset of the dataset for just one epoch and with a low LoRA rank value. To get better responses from the fine-tuned model, you can experiment with:\n",
        "\n",
        "1. Increasing the size of the fine-tuning dataset\n",
        "2. Training for more steps (epochs)\n",
        "3. Setting a higher LoRA rank ✔\n",
        "4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqn89qsz9Mus"
      },
      "source": [
        "## --> roughplay 1 : Rank 4 to 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z2m2YQa9Muv"
      },
      "source": [
        "### Configure LoRA tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lS4ySEFw9Muv"
      },
      "outputs": [],
      "source": [
        "# It seems like rerunning the cell directly caused an error because LoRA was already enabled on the gemma_lm model instance. To change the LoRA rank, you need to reload the model first to get a fresh instance without LoRA enabled.\n",
        "\n",
        "#Here's how you can do it:\n",
        "\n",
        "#Reload the model: Run the code cell below to load the gemma_lm model again.\n",
        "\n",
        "#Change the LoRA rank: Modify the rank value in the LoRA configuration cell (RCucu6oHz53G) to your desired value.\n",
        "\n",
        "#Rerun the subsequent cells: Execute the cells for configuring LoRA (RCucu6oHz53G), displaying the summary (KqYyS0gm6pNy), configuring fine-tuning (p9sBNH8SAjgB), running the fine-tuning (_Peq7TnLtHse), and the inference cells (Y7cDJHy8WfCB and X-2sYl2jqwl7).\n",
        "\n",
        "\n",
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
        "# You can change the rank value below to experiment with different ranks.\n",
        "gemma_lm.backbone.enable_lora(rank=8) # Changed rank from 4 to 8 as an example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LVB-m7h9Muw"
      },
      "source": [
        "Check the model summary after setting the LoRA rank. Notice that enabling LoRA reduces the number of trainable parameters significantly compared to the total number of parameters in the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "de2c2d16-6979-4e8f-97f0-e01a06ab3368",
        "id": "biNnnykO9Muw"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │   \u001b[38;5;34m1,001,190,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,001,190,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,001,190,528\u001b[0m (3.73 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,001,190,528</span> (3.73 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,304,576\u001b[0m (4.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,304,576</span> (4.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEh7Gxop9Muw"
      },
      "source": [
        "Configure the rest of the fine-tuning settings, including the preprocessor settings, optimizer, number of tuning epochs, and batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "nEv0jkqF9Muw"
      },
      "outputs": [],
      "source": [
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG9yA6Fb9Mux"
      },
      "source": [
        "### Run the fine-tune process\n",
        "\n",
        "Run the fine-tuning process using the `fit()` method. This process can take several minutes depending on your compute resources, data size, and number of epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "outputId": "f2219a9d-d12c-4ff9-dd46-f14686c3cad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR33CG2d9Mux"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 323ms/step - loss: 0.7279 - sparse_categorical_accuracy: 0.4965\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fe11420ac00>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEdNLusl9T9z"
      },
      "source": [
        "### Inference after fine-tuning\n",
        "\n",
        "After fine-tuning, you should see changes in the responses when the tuned model is given the same prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRUM4rBY9T90"
      },
      "source": [
        "#### Europe trip prompt\n",
        "\n",
        "Try the Europe trip prompt from earlier and note the differences in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "outputId": "fea2c35e-8b41-44dd-a1be-34729f99c3df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlwWXn5U9T90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What should I do on a trip to Europe?\n",
            "\n",
            "Response:\n",
            "Europe is a continent that offers a variety of travel options, from the beaches of the Mediterranean to the Alps of the European north, from the historic city of Rome to the vibrant city of Barcelona.Europe is also a great place for cultural experiences, such as visiting the Eiffel Tower in Paris, visiting the Colosseum in Rome, or visiting the Great Wall of China.\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_hub.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRTftuVS9T90"
      },
      "source": [
        "The model now provides a shorter response to a question about visiting Europe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PhCfAE89T91"
      },
      "source": [
        "#### Photosynthesis prompt\n",
        "\n",
        "Try the photosynthesis explanation prompt from earlier and note the differences in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "outputId": "c574cc0f-cb5d-4a3e-cf22-3524b5536af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOVROa1v9T91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Explain the process of photosynthesis in a way that a child could understand.\n",
            "\n",
            "Response:\n",
            "Photosynthesis is how plants make their own food.  It is like a magic trick!  Plants take in carbon dioxide (which we breathe out) from the air, like breathing in. They also take in water from the ground through their roots.  Then, with the help of the sun, the plant mixes the carbon dioxide and water together to create sugar.  This sugar is food for the plant.  And, as a bonus, the plants also release oxygen into the air for us to breathe. So, plants help us breathe, and they make their own food.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --> roughplay 2 : Dataset size change"
      ],
      "metadata": {
        "id": "I6mwaldiHK09"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HXsXGZPHE2_"
      },
      "source": [
        "### Format tuning data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "qyliEwjdHE3A"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "prompts = []\n",
        "responses = []\n",
        "line_count = 0\n",
        "\n",
        "with open(\"databricks-dolly-15k.jsonl\") as file:\n",
        "    for line in file:\n",
        "        if line_count >= 1500:\n",
        "            break  # Limit the training examples, to reduce execution time.\n",
        "\n",
        "        examples = json.loads(line)\n",
        "        # Filter out examples with context, to keep it simple.\n",
        "        if examples[\"context\"]:\n",
        "            continue\n",
        "        # Format data into prompts and response lists.\n",
        "        prompts.append(examples[\"instruction\"])\n",
        "        responses.append(examples[\"response\"])\n",
        "\n",
        "        line_count += 1\n",
        "\n",
        "data = {\n",
        "    \"prompts\": prompts,\n",
        "    \"responses\": responses\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcdlEZTHiZS"
      },
      "source": [
        "### Configure LoRA tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3Ns24Mm7HiZT"
      },
      "outputs": [],
      "source": [
        "# It seems like rerunning the cell directly caused an error because LoRA was already enabled on the gemma_lm model instance. To change the LoRA rank, you need to reload the model first to get a fresh instance without LoRA enabled.\n",
        "\n",
        "#Here's how you can do it:\n",
        "\n",
        "#Reload the model: Run the code cell below to load the gemma_lm model again.\n",
        "\n",
        "#Change the LoRA rank: Modify the rank value in the LoRA configuration cell (RCucu6oHz53G) to your desired value.\n",
        "\n",
        "#Rerun the subsequent cells: Execute the cells for configuring LoRA (RCucu6oHz53G), displaying the summary (KqYyS0gm6pNy), configuring fine-tuning (p9sBNH8SAjgB), running the fine-tuning (_Peq7TnLtHse), and the inference cells (Y7cDJHy8WfCB and X-2sYl2jqwl7).\n",
        "\n",
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
        "# You can change the rank value below to experiment with different ranks.\n",
        "gemma_lm.backbone.enable_lora(rank=4) # Changed rank from 4 to 8 as an example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVXROykPHiZT"
      },
      "source": [
        "Check the model summary after setting the LoRA rank. Notice that enabling LoRA reduces the number of trainable parameters significantly compared to the total number of parameters in the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "acf90ca4-2278-4932-e51e-649d56e76528",
        "collapsed": true,
        "id": "qAyD-7b0HiZT"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │   \u001b[38;5;34m1,000,538,240\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,538,240</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,538,240\u001b[0m (3.73 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,538,240</span> (3.73 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m652,288\u001b[0m (2.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">652,288</span> (2.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKCMvmXWHiZT"
      },
      "source": [
        "Configure the rest of the fine-tuning settings, including the preprocessor settings, optimizer, number of tuning epochs, and batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kHVlscmRHiZT"
      },
      "outputs": [],
      "source": [
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idwfhjxDHiZU"
      },
      "source": [
        "### Run the fine-tune process\n",
        "\n",
        "Run the fine-tuning process using the `fit()` method. This process can take several minutes depending on your compute resources, data size, and number of epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "outputId": "bba4d0fc-e67a-4e38-e2e7-24394937b886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PcuYjQaHiZU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 321ms/step - loss: 0.7162 - sparse_categorical_accuracy: 0.4988\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fe125cad010>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKBdBY1NHiZU"
      },
      "source": [
        "### Inference after fine-tuning\n",
        "\n",
        "After fine-tuning, you should see changes in the responses when the tuned model is given the same prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcDZtGn5HiZU"
      },
      "source": [
        "#### Europe trip prompt\n",
        "\n",
        "Try the Europe trip prompt from earlier and note the differences in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "outputId": "4ddd00a6-8762-4f9c-d28b-eec0a8261660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAnrfdX3HiZU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What should I do on a trip to Europe?\n",
            "\n",
            "Response:\n",
            "Europe has many attractions. You could visit the capital cities such as London, Paris, Rome, and Madrid. You could explore historical cities such as Prague, Florence, and Berlin.  You could also visit the natural beauty of Europe such as the Alps, the Alps, and the Pyrenees. You could visit a museum, such as the British Museum or the Louvre, or you could go for a bike ride along the canals of Venice.\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_hub.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy3wYWsZHiZV"
      },
      "source": [
        "The model now provides a shorter response to a question about visiting Europe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j3r1uXXHiZV"
      },
      "source": [
        "#### Photosynthesis prompt\n",
        "\n",
        "Try the photosynthesis explanation prompt from earlier and note the differences in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "outputId": "fd64c6a6-f7d1-43db-cd88-f0d4e090ac1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVtFBt1CHiZV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "Explain the process of photosynthesis in a way that a child could understand.\n",
            "\n",
            "Response:\n",
            "Photosynthesis is how plants make their own food! It's like a recipe.  Plants use the sun, water, and air to make their food. First, they take in water through their roots and they take in carbon dioxide from the air through their leaves. Then, the leaves use the sun, which is the energy source. The plants use water to make sugar for energy, and they release the carbon dioxide back into the air.  The carbon dioxide is the air that we breathe out.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSsRdeiof_rJ"
      },
      "source": [
        "## Summary and next steps\n",
        "\n",
        "This tutorial covered LoRA fine-tuning on a Gemma model using Keras. Check out the following docs next:\n",
        "\n",
        "* Learn how to [generate text with a Gemma model](https://ai.google.dev/gemma/docs/get_started).\n",
        "* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/core/distributed_tuning).\n",
        "* Learn how to [use Gemma open models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma).\n",
        "* Learn how to [fine-tune Gemma using Keras and deploy to Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SDEExiAk4fLb",
        "I_1BItm1CcZ8",
        "lyhHCMfoRZ_v",
        "7iOF6Yo-wUEC",
        "CuEUAKJW1QkQ",
        "rGLS-l5TxIR4",
        "hZs8XXqUKRmi",
        "7RCE3fdGhDE5",
        "G_L6A5J-1QgC",
        "4h4fy-3003tI",
        "YQ74Zz_S0iVv",
        "9T7xe_jzslv4",
        "45UpBDfBgf0I",
        "cBLW5hiGj31i",
        "OA0ozGC66tk1",
        "bx3m8f1dB7nk",
        "4yd-1cNw1dTn",
        "H55JYJ1a1Kos",
        "H7nVd8Mi1Yta",
        "mqn89qsz9Mus",
        "-Z2m2YQa9Muv",
        "KRUM4rBY9T90",
        "1HXsXGZPHE2_",
        "QmcdlEZTHiZS",
        "lKBdBY1NHiZU",
        "FcDZtGn5HiZU",
        "7j3r1uXXHiZV",
        "gSsRdeiof_rJ"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
