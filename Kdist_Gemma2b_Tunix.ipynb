{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"last_runtime":{"build_target":"//learning/grp/tools/ml_python:ml_notebook","kind":"private"},"private_outputs":true,"provenance":[]},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":26131,"sourceType":"modelInstanceVersion","modelInstanceId":21997,"modelId":3301},{"sourceId":26141,"sourceType":"modelInstanceVersion","modelInstanceId":22004,"modelId":3301}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cscpsb/knowledge-distillation-gemma2bflex-using-tunix?scriptVersionId=268507662\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Knowledge Distillation with Tunix: Gemma 7B to Gemma 2B\n\n## Install necessary libraries\nThe core components we'll use are:\n-   **Teacher Model**: Gemma 7B\n-   **Student Model**: Gemma 2B\n-   **Distillation Strategy**: `tunix.distillation.strategies.LogitStrategy`\n-   **Trainer**: `tunix.distillation.DistillationTrainer`\n\nIn this tutorial, we use a v5e-8 TPU. Let's get started!","metadata":{"id":"0hBMTLEWzVro"}},{"cell_type":"code","source":"# Imports\n\nimport os\n\nos.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n\nimport gc\nimport logging\n\nimport datasets\nfrom flax import nnx\nimport huggingface_hub\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nfrom tunix.distillation import distillation_trainer\nfrom tunix.distillation import strategies\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\nfrom tunix.models.gemma import model as gemma_lib\nfrom tunix.models.gemma import params as params_lib\nfrom tunix.examples.data import translation_dataset as data_lib\n\n\n\n# Disable interactive progress bar for Kaggle\nhuggingface_hub.utils.disable_progress_bars()\n\nlog_handler = logging.StreamHandler()\nlogging.root.addHandler(log_handler)\n","metadata":{"id":"c1xPRviCzdgg","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:31:24.211819Z","iopub.execute_input":"2025-10-16T15:31:24.211976Z","iopub.status.idle":"2025-10-16T15:31:50.004514Z","shell.execute_reply.started":"2025-10-16T15:31:24.211959Z","shell.execute_reply":"2025-10-16T15:31:50.0035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utility Function to check HBM","metadata":{"id":"u0xLXuslziAm"}},{"cell_type":"code","source":"import functools\nimport humanize\nfrom tunix.sft import utils\n\nshow_hbm_usage = utils.show_hbm_usage\nshow_hbm_usage()","metadata":{"id":"cNgOJM4Yzma7","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:31:50.00501Z","iopub.execute_input":"2025-10-16T15:31:50.005469Z","iopub.status.idle":"2025-10-16T15:32:01.353386Z","shell.execute_reply.started":"2025-10-16T15:31:50.00545Z","shell.execute_reply":"2025-10-16T15:32:01.352194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Data ---\nBATCH_SIZE = 4\nMAX_TARGET_LENGTH = 128\nNUM_TRAIN_EPOCHS = 1\n\n# --- Model ---\nMESH = [(1, 8), (\"fsdp\", \"tp\")]\n\n# --- Training ---\nMAX_STEPS = 200\nEVAL_EVERY_N_STEPS = 50\nLEARNING_RATE = 1e-4\n\n# --- Distillation ---\nTEMPERATURE = 2.0  # Softens the teacher's probabilities\nALPHA = 0.7  # Balances distillation loss and student's own task loss\n\n# --- Checkpointing ---\nTEACHER_CKPT_DIR = \"/content/intermediate_ckpt/teacher/\"\nSTUDENT_CKPT_DIR = \"/content/intermediate_ckpt/student/\"","metadata":{"id":"KPVVoafkzUhe","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:32:01.354132Z","iopub.execute_input":"2025-10-16T15:32:01.354361Z","iopub.status.idle":"2025-10-16T15:32:01.358302Z","shell.execute_reply.started":"2025-10-16T15:32:01.354342Z","shell.execute_reply":"2025-10-16T15:32:01.357461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First, we need to load our teacher and student models. We'll use Gemma 7B as the teacher and Gemma 2B as the student.\n\n**Important**: You must have a Kaggle account and agree to the Gemma license to download the models. The first time you run this, you will be prompted to log in to Kaggle.","metadata":{"id":"QyHsZZ7hzuBl"}},{"cell_type":"code","source":"# Log in to Kaggle\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n  kagglehub.login()\n\n\ndef load_and_save_model(model_handle, version, ckpt_dir):\n  \"\"\"Loads a model from Kaggle, saves it locally, and cleans up memory.\"\"\"\n  print(f\"Loading {model_handle}...\")\n  kaggle_ckpt_path = kagglehub.model_download(model_handle)\n  ckpt_version = \"2b-it\"\n  if \"7b\" in version:\n    ckpt_version = \"7b-it\"\n  # Temporarily set the default device to CPU for loading the full model\n  with jax.default_device(jax.devices(\"cpu\")[0]):\n    params = params_lib.load_and_format_params(\n        os.path.join(kaggle_ckpt_path, ckpt_version)\n    )\n    gemma = gemma_lib.Transformer.from_params(params, version=version)\n\n  print(f\"Saving checkpoint to {ckpt_dir}...\")\n  checkpointer = ocp.StandardCheckpointer()\n  _, state = nnx.split(gemma)\n  checkpointer.save(os.path.join(ckpt_dir, \"state\"), state)\n  checkpointer.wait_until_finished()\n  # Clean up to save memory\n  del params\n  del gemma\n  del state\n  gc.collect()\n  print(f\"Finished processing {model_handle}.\")\n\n\n# Load Teacher Model (Gemma 7B)\nload_and_save_model(\n    \"google/gemma/flax/1.1-7b-it\", \"1.1-7b-it\", TEACHER_CKPT_DIR\n)\n\n# Load Student Model (Gemma 2B)\nload_and_save_model(\n    \"google/gemma/flax/1.1-2b-it\", \"1.1-2b-it\", STUDENT_CKPT_DIR\n)","metadata":{"id":"h1iQ5hzuzrbR","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:32:01.358844Z","iopub.execute_input":"2025-10-16T15:32:01.35901Z","iopub.status.idle":"2025-10-16T15:35:39.238166Z","shell.execute_reply.started":"2025-10-16T15:32:01.358997Z","shell.execute_reply":"2025-10-16T15:35:39.237014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have the checkpoints saved locally, we can load them into sharded models. Sharding is essential for training large models efficiently on TPUs by distributing the model's weights and the computation across multiple devices.","metadata":{"id":"ALVsADIVz4Iw"}},{"cell_type":"code","source":"def get_sharded_model(ckpt_path, model_config, mesh):\n  \"\"\"Loads a checkpoint into a sharded model.\"\"\"\n  abs_gemma: nnx.Module = nnx.eval_shape(\n      lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n  )\n  abs_state = nnx.state(abs_gemma)\n  abs_state = jax.tree.map(\n      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n      abs_state,\n      nnx.get_named_sharding(abs_state, mesh),\n  )\n  checkpointer = ocp.StandardCheckpointer()\n  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n\n  graph_def, _ = nnx.split(abs_gemma)\n  gemma = nnx.merge(graph_def, restored_params)\n  return gemma\n\n\nmesh = jax.make_mesh(*MESH)\n\n# Create Teacher Model\nprint(\"Creating sharded teacher model (Gemma 7B)...\")\nteacher_config = gemma_lib.ModelConfig.gemma_7b()\nteacher_model = get_sharded_model(\n    os.path.join(TEACHER_CKPT_DIR, \"state\"), teacher_config, mesh\n)\nprint(\"Teacher model created.\")\n# nnx.display(teacher_model) # Optional: view model structure\n\n# Create Student Model\nprint(\"\\nCreating sharded student model (Gemma 2B)...\")\nstudent_config = gemma_lib.ModelConfig.gemma_2b()\nstudent_model = get_sharded_model(\n    os.path.join(STUDENT_CKPT_DIR, \"state\"), student_config, mesh\n)\nprint(\"Student model created.\")\n# nnx.display(student_model) # Optional: view model structure\n\nshow_hbm_usage()","metadata":{"id":"ZNC5c8glz3Mu","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:35:39.238679Z","iopub.execute_input":"2025-10-16T15:35:39.238872Z","iopub.status.idle":"2025-10-16T15:35:49.949665Z","shell.execute_reply.started":"2025-10-16T15:35:39.238856Z","shell.execute_reply":"2025-10-16T15:35:49.948506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading tokenizer...\")\ngemma_tokenizer_path = os.path.join(\n    kagglehub.model_download(\"google/gemma/flax/1.1-2b-it\"), \"tokenizer.model\"\n)\ngemma_tokenizer = tokenizer_lib.Tokenizer(\n    tokenizer_type=\"sentencepiece\",\n    tokenizer_path=gemma_tokenizer_path)\nprint(\"Tokenizer loaded.\")\n","metadata":{"id":"ixCv3zQO0CeD","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:35:49.950088Z","iopub.execute_input":"2025-10-16T15:35:49.950312Z","iopub.status.idle":"2025-10-16T15:35:50.518221Z","shell.execute_reply.started":"2025-10-16T15:35:49.95029Z","shell.execute_reply":"2025-10-16T15:35:50.516982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\n\n\nprint(\"Loading dataset...\")\nDATA_DIR = \"/kaggle/input/en-fr-translation-dataset\"\n\ntrain_ds, validation_ds = data_lib.create_datasets(\n    dataset_name='Helsinki-NLP/opus-100',\n    global_batch_size=16,\n    max_target_length=256,\n    num_train_epochs=1,\n    tokenizer=gemma_tokenizer,\n    instruct_tuned=True,\n)\n\nprint(\"Dataset loaded!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:35:50.518765Z","iopub.execute_input":"2025-10-16T15:35:50.518957Z","iopub.status.idle":"2025-10-16T15:35:54.298048Z","shell.execute_reply.started":"2025-10-16T15:35:50.518941Z","shell.execute_reply":"2025-10-16T15:35:54.29698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `LogitStrategy` requires three key functions:\n1.  `model_forward_fn`: A function that performs a forward pass for a given model and returns its logits. Since both our models are from the Gemma family, we can use the same function for both.\n2.  `labels_fn`: A function that creates the ground-truth labels from the input data for the standard cross-entropy loss.\n3.  `gen_model_input_fn`: A helper function to format each batch from the data loader into the dictionary format expected by the model.","metadata":{"id":"qHdFCIT00H80"}},{"cell_type":"code","source":"VOCAB_SIZE = student_config.num_embed\n\n\ndef model_forward_fn(\n    model: nnx.Module,\n    input_tokens: jax.Array,\n    input_mask: jax.Array,\n    positions: jax.Array,\n    attention_mask: jax.Array,\n):\n  \"\"\"Performs a forward pass and returns the logits.\"\"\"\n  logits, _ = model(\n      input_tokens,\n      positions,\n      None,\n      attention_mask,\n  )\n  # Exclude the last step as it does not appear in the targets.\n  return logits[:, :-1, :]\n\n\ndef labels_fn(\n    input_tokens: jax.Array,\n    input_mask: jax.Array,\n    **kwargs,\n):\n  \"\"\"Creates one-hot encoded labels for the next-token prediction task.\"\"\"\n  target_tokens = input_tokens[:, 1:]\n  target_mask = input_mask[:, 1:]\n  labels = jax.nn.one_hot(target_tokens, VOCAB_SIZE)\n  # Mask out the padding tokens from the loss calculation.\n  return labels * target_mask.astype(labels.dtype)[..., None]\n\n\ndef gen_model_input_fn(x: distillation_trainer.TrainingInput):\n  \"\"\"Formats a batch from the data loader into the model's expected input format.\"\"\"\n  pad_mask = x.input_tokens != gemma_tokenizer.pad_id()\n  positions = utils.build_positions_from_mask(pad_mask)\n  attention_mask = utils.make_causal_attn_mask(pad_mask)\n  return {\n      'input_tokens': x.input_tokens,\n      'input_mask': x.input_mask,\n      'positions': positions,\n      'attention_mask': attention_mask,\n  }\n\n","metadata":{"id":"cEez-UGO0Isd","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:35:54.298514Z","iopub.execute_input":"2025-10-16T15:35:54.298974Z","iopub.status.idle":"2025-10-16T15:35:54.304265Z","shell.execute_reply.started":"2025-10-16T15:35:54.298957Z","shell.execute_reply":"2025-10-16T15:35:54.303387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can assemble all the components. We'll instantiate the `LogitStrategy`, configure the `DistillationTrainer`, and start the training process. The trainer will handle the distributed training loop across the available TPU cores.","metadata":{"id":"t43x9Fla0S0X"}},{"cell_type":"code","source":"# 1. Setup the distillation strategy\nstrategy = strategies.LogitStrategy(\n    student_forward_fn=model_forward_fn,\n    teacher_forward_fn=model_forward_fn,\n    labels_fn=labels_fn,\n    temperature=TEMPERATURE,\n    alpha=ALPHA,\n)\n\n# 2. Setup the training configuration\nconfig = distillation_trainer.TrainingConfig(\n    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n    max_steps=MAX_STEPS,\n)\n\n# 3. Setup the optimizer\noptimizer = optax.adamw(LEARNING_RATE)\n\n\n# Set teacher model in eval mode\nteacher_model.eval()\n# Set student model in train mode\nstudent_model.train()\n# 4. Instantiate the trainer\ntrainer = distillation_trainer.DistillationTrainer(\n    student_model=student_model,\n    teacher_model=teacher_model,\n    strategy=strategy,\n    optimizer=optimizer,\n    training_config=config,\n).with_gen_model_input_fn(gen_model_input_fn)\n\n\n# 5. Run training within the mesh context, the first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per, please open a bug. Really appreciated!\nprint(\"Starting distillation training...\")\nwith mesh:\n  trainer.train(train_ds, validation_ds)\nprint(\"Training complete.\")","metadata":{"id":"XBugUhCl0VwK","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:35:54.304666Z","iopub.execute_input":"2025-10-16T15:35:54.30483Z","iopub.status.idle":"2025-10-16T17:01:09.883821Z","shell.execute_reply.started":"2025-10-16T15:35:54.304817Z","shell.execute_reply":"2025-10-16T17:01:09.882882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Student Model","metadata":{}},{"cell_type":"code","source":"import os\nimport orbax.checkpoint as ocp\n#import nnx\n\n# --- After 'trainer.train' completes ---\nprint(\"Training complete.\")\n\n# 1. Define the absolute path for the final saved model\n# The output folder in Kaggle kernels is /kaggle/working\nKAGGLE_WORKING_DIR = \"/kaggle/working\"\nFINAL_MODEL_SAVE_DIR_NAME = \"final_distilled_gemma_checkpoint\"\n\n# Join the working directory with your desired folder name to create an ABSOLUTE path\nFINAL_MODEL_SAVE_PATH = os.path.join(KAGGLE_WORKING_DIR, FINAL_MODEL_SAVE_DIR_NAME)\nos.makedirs(FINAL_MODEL_SAVE_PATH, exist_ok=True) # Create the directory\n\n# 2. Extract the final state from the **original student_model variable**\n# This variable holds the model whose parameters were updated in-place by the trainer.\n# We discard the model object (_) and keep the updated state.\n_, final_student_state = nnx.split(student_model)\n\n# 3. Save the state using the OCP checkpointer\nprint(f\"Saving final trained student checkpoint to ABSOLUTE PATH: {FINAL_MODEL_SAVE_PATH}...\")\ncheckpointer = ocp.StandardCheckpointer()\n\n# The save path is the absolute path to your directory\ncheckpointer.save(os.path.join(FINAL_MODEL_SAVE_PATH, \"trained_state\"), final_student_state)\ncheckpointer.wait_until_finished() # Wait for the save operation to complete\n\nprint(f\"Successfully saved final student model to {FINAL_MODEL_SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:13:14.797639Z","iopub.execute_input":"2025-10-16T17:13:14.797944Z","iopub.status.idle":"2025-10-16T17:13:51.002861Z","shell.execute_reply.started":"2025-10-16T17:13:14.797925Z","shell.execute_reply":"2025-10-16T17:13:51.00155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a Kaggle Dataset (Recommended):\nGo to the committed version of your training notebook.\nNavigate to the Output tab.\nFind the saved output folder (final_distilled_gemma_checkpoint).\nClick the \"Create New Dataset\" button (or similar option) next to the output.\nGive your dataset a clear name (e.g., gemma-2b-distilled-checkpoint).\nThis makes your model checkpoint a shareable, versioned Kaggle Dataset that can be added as an input source to any other notebook.","metadata":{}},{"cell_type":"markdown","source":"## Quick Eval\nAfter training, the student model should have improved its ability to perform the translation task by learning from the teacher. Let's test it with a few sample prompts.","metadata":{"id":"Z1_1VkkB0ev_"}},{"cell_type":"code","source":"print(\"Setting up sampler for evaluation...\")\nsampler = sampler_lib.Sampler(\n    transformer=student_model,\n    tokenizer=gemma_tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_TARGET_LENGTH + 64,\n        num_layers=student_config.num_layers,\n        num_kv_heads=student_config.num_kv_heads,\n        head_dim=student_config.head_dim,\n    ),\n)","metadata":{"id":"Rn_V4rw70fXL","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:01:09.88416Z","iopub.execute_input":"2025-10-16T17:01:09.88433Z","iopub.status.idle":"2025-10-16T17:01:09.907243Z","shell.execute_reply.started":"2025-10-16T17:01:09.884315Z","shell.execute_reply":"2025-10-16T17:01:09.906258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_batch = [\n    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n    \"Translate this into French:\\nThis dish is delicious!\\n\",\n    \"Translate this into French:\\nI am a student.\\n\",\n]\n\nprint(\"Generating translations with the distilled student model...\")\nwith mesh:\n  out_data = sampler(\n      input_strings=input_batch,\n      max_generation_steps=20,\n  )\n\nprint(\"\\n--- Evaluation Results ---\")\nfor input_string, out_string in zip(input_batch, out_data.text):\n  print(f\"----------------------\")\n  print(f\"Prompt:\\n{input_string}\")\n  print(f\"Distilled Student's Output:\\n{out_string}\")","metadata":{"id":"13ozlAZP0jl4","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:01:09.90756Z","iopub.execute_input":"2025-10-16T17:01:09.907717Z","iopub.status.idle":"2025-10-16T17:01:22.147963Z","shell.execute_reply.started":"2025-10-16T17:01:09.907703Z","shell.execute_reply":"2025-10-16T17:01:22.146904Z"}},"outputs":[],"execution_count":null}]}
